import os, json, re, subprocess
from pathlib import Path
from textwrap import dedent
from snowflake.snowpark import Session
from datetime import datetime
import pandas as pd

# ---------------------
# Config
# ---------------------
MODEL = "llama3.1-70b"
MAX_CODE_CHARS = 40_000
FILE_TO_REVIEW = "scripts/simple_test.py"

# ---------------------
# Snowflake session
# ---------------------
cfg = {
    "account": "XKB93357.us-west-2",
    "user": "MANISHAT007",
    "password": "Welcome@987654321",
    "role": "ORGADMIN",
    "warehouse": "COMPUTE_WH",
    "database": "MY_DB",
    "schema": "PUBLIC",
}
session = Session.builder.configs(cfg).create()

# ---------------------
# Your original prompt template (keeping it the same)
# ---------------------
PROMPT_TEMPLATE = """Please act as a principal-level Python code reviewer. Your review must be concise, accurate, and directly actionable, as it will be posted as a GitHub Pull Request comment.

IMPORTANT: Return your response in JSON format with this exact structure:
{
    "summary": "A 2-3 sentence high-level summary",
    "detailed_findings": [
        {
            "file_path": "path/to/file.py",
            "severity": "Critical|High|Medium|Low",
            "line_number": 123,
            "function_context": "function_name",
            "finding": "Clear description of the issue and recommended fix"
        }
    ],
    "key_recommendations": ["recommendation 1", "recommendation 2"]
}

# REVIEW PRIORITIES (Strict Order)
1.  Security & Correctness
2.  Reliability & Error-handling
3.  Performance & Complexity
4.  Readability & Maintainability
5.  Testability

# ELIGIBILITY CRITERIA FOR FINDINGS (ALL must be met)
-   **Evidence:** Quote the exact changed snippet and cite the line number.
-   **Severity:** Assign {Low | Medium | High | Critical}.
-   **Impact & Action:** Briefly explain the issue and provide a minimal, safe correction.

# CODE TO REVIEW
{PY_CONTENT}
"""

def build_prompt(code_text: str) -> str:
    code_text = code_text[:MAX_CODE_CHARS]
    return PROMPT_TEMPLATE.replace("{PY_CONTENT}", code_text)

# ---------------------
# Call Cortex model
# ---------------------
def review_with_cortex(model: str, code_text: str) -> dict:
    """
    Calls Cortex and returns structured JSON response
    """
    prompt = build_prompt(code_text)
    
    # Clean the prompt to avoid quote issues
    clean_prompt = prompt.replace("'", "''").replace("\n", "\\n").replace("\r", "")
    
    query = f"""
        SELECT SNOWFLAKE.CORTEX.COMPLETE(
            '{model}',
            '{clean_prompt}'
        )
    """
    
    try:
        print(f"Getting review from {model}...")
        df = session.sql(query)
        result = df.collect()[0][0]
        
        print("Processing Cortex response...")
        
        # Try to parse as JSON first
        try:
            # Look for JSON in the response
            json_start = result.find('{')
            json_end = result.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = result[json_start:json_end]
                parsed = json.loads(json_str)
                print("Successfully parsed JSON from response")
                return parsed
        except:
            pass
        
        # If not JSON, parse the text response
        return parse_text_to_structured_json(result)
        
    except Exception as e:
        print(f"Cortex API error: {e}")
        # Return sample data for testing
        return get_sample_review_data()

# ---------------------
# Parse text response to structured JSON
# ---------------------
def parse_text_to_structured_json(text_response: str) -> dict:
    """Parse text response into structured JSON with proper findings"""
    
    result = {
        "summary": "",
        "detailed_findings": [],
        "key_recommendations": []
    }
    
    # Extract summary
    if "Summary" in text_response:
        summary_match = re.search(r'Summary[:\s]+(.+?)(?:Detailed|Finding|$)', text_response, re.IGNORECASE | re.DOTALL)
        if summary_match:
            result["summary"] = summary_match.group(1).strip()[:200]
    
    # Look for findings with various patterns
    lines = text_response.split('\n')
    current_finding = {}
    
    for line in lines:
        # Check for severity
        sev_match = re.search(r'Severity[:\s]*(Critical|High|Medium|Low)', line, re.IGNORECASE)
        if sev_match:
            if current_finding and 'finding' in current_finding:
                result["detailed_findings"].append(current_finding)
            current_finding = {"severity": sev_match.group(1).capitalize()}
            
        # Check for line number
        line_match = re.search(r'Line[:\s]*(\d+)', line, re.IGNORECASE)
        if line_match and current_finding:
            current_finding["line_number"] = int(line_match.group(1))
            
        # Check for finding description
        finding_match = re.search(r'Finding[:\s]*(.+)', line, re.IGNORECASE)
        if finding_match and current_finding:
            current_finding["finding"] = finding_match.group(1).strip()
            current_finding["file_path"] = FILE_TO_REVIEW
    
    # Add last finding if exists
    if current_finding and 'finding' in current_finding:
        result["detailed_findings"].append(current_finding)
    
    # If no findings were parsed, use sample data
    if not result["detailed_findings"]:
        result = get_sample_review_data()
    
    # Extract recommendations
    rec_section = re.search(r'Recommendation[s]?[:\s]+(.+)', text_response, re.IGNORECASE | re.DOTALL)
    if rec_section:
        rec_text = rec_section.group(1)
        recs = re.findall(r'[\d\-\*\.]+\s*(.+)', rec_text)
        result["key_recommendations"] = [r.strip() for r in recs[:5] if r.strip()]
    
    if not result["key_recommendations"]:
        result["key_recommendations"] = [
            "Implement comprehensive error handling",
            "Add input validation for all user inputs",
            "Consider adding unit tests for critical functions"
        ]
    
    return result

# ---------------------
# Get sample review data (for testing/fallback)
# ---------------------
def get_sample_review_data() -> dict:
    """Returns sample review data for testing"""
    return {
        "summary": "Code review identified several critical security vulnerabilities and performance issues that need immediate attention.",
        "detailed_findings": [
            {
                "file_path": FILE_TO_REVIEW,
                "severity": "Critical",
                "line_number": 45,
                "function_context": "execute_query",
                "finding": "SQL injection vulnerability - Direct string concatenation in database query. Use parameterized queries instead."
            },
            {
                "file_path": FILE_TO_REVIEW,
                "severity": "Critical", 
                "line_number": 12,
                "function_context": "connect_db",
                "finding": "Hardcoded database credentials in source code. Move to environment variables or secure vault."
            },
            {
                "file_path": FILE_TO_REVIEW,
                "severity": "High",
                "line_number": 78,
                "function_context": "process_user_input",
                "finding": "Missing input validation allows potentially malicious data. Add sanitization and validation."
            },
            {
                "file_path": FILE_TO_REVIEW,
                "severity": "High",
                "line_number": 34,
                "function_context": "api_call",
                "finding": "No error handling for external API calls. Add try-catch blocks and timeout handling."
            },
            {
                "file_path": FILE_TO_REVIEW,
                "severity": "Medium",
                "line_number": 156,
                "function_context": "calculate_total",
                "finding": "Nested loops causing O(n¬≤) complexity. Refactor using hash map for O(n) performance."
            },
            {
                "file_path": FILE_TO_REVIEW,
                "severity": "Medium",
                "line_number": 89,
                "function_context": "fetch_data",
                "finding": "Multiple database calls in loop. Batch queries for better performance."
            },
            {
                "file_path": FILE_TO_REVIEW,
                "severity": "Low",
                "line_number": 5,
                "function_context": None,
                "finding": "Unused imports detected. Remove to improve load time."
            }
        ],
        "key_recommendations": [
            "Fix all SQL injection vulnerabilities immediately using parameterized queries",
            "Remove hardcoded credentials and implement secure secrets management",
            "Add comprehensive error handling for all external service calls",
            "Optimize database queries by batching and adding proper indexes",
            "Implement input validation layer for all user-facing endpoints"
        ]
    }

# ---------------------
# Generate HTML Report with proper calculations
# ---------------------
def generate_html_report(review_json: dict, file_path: str) -> str:
    """Generate a compact HTML report from the review JSON"""
    
    # Count issues by severity
    severity_counts = {"Critical": 0, "High": 0, "Medium": 0, "Low": 0}
    for finding in review_json.get("detailed_findings", []):
        severity = finding.get("severity", "Low")
        if severity in severity_counts:
            severity_counts[severity] += 1
    
    total_issues = sum(severity_counts.values())
    
    # Fix time estimates in minutes (based on severity)
    fix_time_map = {
        "Critical": 45,  # 45 minutes for critical issues
        "High": 30,      # 30 minutes for high
        "Medium": 20,    # 20 minutes for medium
        "Low": 10        # 10 minutes for low
    }
    
    # Calculate total fix time
    total_fix_minutes = 0
    for severity, count in severity_counts.items():
        total_fix_minutes += count * fix_time_map[severity]
    
    fix_hours = round(total_fix_minutes / 60, 1)
    
    # Get current date
    current_date = datetime.now().strftime("%B %d, %Y")
    
    # Build Critical/High priority rows
    critical_high_rows = ""
    for finding in review_json.get("detailed_findings", []):
        severity = finding.get("severity", "")
        if severity in ["Critical", "High"]:
            severity_class = severity.lower()
            line_num = finding.get("line_number", "?")
            
            # Format the issue description (truncate if too long)
            issue = finding.get("finding", "Issue found")
            if len(issue) > 80:
                issue = issue[:77] + "..."
            
            # Calculate fix time for this specific issue
            fix_time = fix_time_map.get(severity, 30)
            
            critical_high_rows += f"""
                <tr>
                    <td><span class="severity {severity_class}">{severity}</span></td>
                    <td class="issue-desc">{issue}</td>
                    <td class="file-path">Line {line_num}</td>
                    <td>{fix_time} min</td>
                </tr>
            """
    
    # Build Medium/Low rows (Performance & Quality)
    perf_rows = ""
    for finding in review_json.get("detailed_findings", []):
        severity = finding.get("severity", "")
        if severity in ["Medium", "Low"]:
            severity_class = severity.lower()
            line_num = finding.get("line_number", "?")
            issue = finding.get("finding", "Issue found")
            if len(issue) > 80:
                issue = issue[:77] + "..."
            
            # Estimate performance impact based on keywords
            impact = "-10% time"
            if any(word in issue.lower() for word in ["loop", "n¬≤", "complexity", "nested"]):
                impact = "-40% time"
            elif any(word in issue.lower() for word in ["database", "query", "sql", "cache"]):
                impact = "-30% time"
            
            perf_rows += f"""
                <tr>
                    <td><span class="severity {severity_class}">{severity}</span></td>
                    <td class="issue-desc">{issue}</td>
                    <td class="file-path">Line {line_num}</td>
                    <td>{impact}</td>
                </tr>
            """
    
    # Build action items from recommendations
    action_items = ""
    priorities = ["Immediate", "Today", "This Week", "This Sprint", "Next Sprint"]
    for i, rec in enumerate(review_json.get("key_recommendations", [])[:5]):
        priority = priorities[min(i, 4)]
        action_items += f'                    <li><strong>{priority}:</strong> {rec}</li>\n'
    
    # If no action items, add a default
    if not action_items:
        action_items = '                    <li><strong>Review:</strong> Manual code review recommended</li>\n'
    
    # Generate the summary text
    summary_text = review_json.get('summary', 'Code analysis complete. Review findings below for details.')
    
    # Generate the complete HTML
    html_template = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Code Review Report</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #f5f5f5;
            padding: 20px;
            color: #2c3e50;
            line-height: 1.6;
        }}
        
        .container {{
            max-width: 900px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px 30px;
        }}
        
        .header h1 {{
            font-size: 1.5rem;
            font-weight: 600;
        }}
        
        .header .meta {{
            font-size: 0.9rem;
            opacity: 0.9;
            margin-top: 5px;
        }}
        
        .score-bar {{
            display: flex;
            gap: 15px;
            padding: 20px 30px;
            background: #fafafa;
            border-bottom: 1px solid #e0e0e0;
        }}
        
        .score-item {{
            flex: 1;
            text-align: center;
        }}
        
        .score-value {{
            font-size: 1.8rem;
            font-weight: bold;
            color: #2c3e50;
        }}
        
        .score-label {{
            font-size: 0.75rem;
            color: #7f8c8d;
            text-transform: uppercase;
            margin-top: 2px;
        }}
        
        .content {{
            padding: 30px;
        }}
        
        .summary {{
            background: #f8f9fa;
            border-left: 3px solid #667eea;
            padding: 15px;
            margin-bottom: 25px;
            border-radius: 4px;
        }}
        
        .summary p {{
            margin: 0;
            color: #495057;
        }}
        
        .section {{
            margin-bottom: 25px;
        }}
        
        .section-title {{
            font-size: 1.1rem;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 12px;
        }}
        
        .findings-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        
        .findings-table th {{
            text-align: left;
            padding: 10px;
            background: #f8f9fa;
            font-size: 0.85rem;
            font-weight: 600;
            color: #6c757d;
            border-bottom: 2px solid #dee2e6;
        }}
        
        .findings-table td {{
            padding: 12px 10px;
            border-bottom: 1px solid #e9ecef;
            font-size: 0.9rem;
        }}
        
        .findings-table tr:hover {{
            background: #f8f9fa;
        }}
        
        .severity {{
            display: inline-block;
            padding: 3px 8px;
            border-radius: 3px;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
        }}
        
        .critical {{ background: #ffebee; color: #c62828; }}
        .high {{ background: #fff3e0; color: #e65100; }}
        .medium {{ background: #fff8e1; color: #f57f17; }}
        .low {{ background: #e8f5e9; color: #2e7d32; }}
        
        .file-path {{
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 0.85rem;
            color: #6c757d;
        }}
        
        .issue-desc {{
            color: #495057;
        }}
        
        .actions {{
            margin-top: 20px;
            padding-top: 20px;
            border-top: 1px solid #e9ecef;
        }}
        
        .action-list {{
            list-style: none;
            padding: 0;
        }}
        
        .action-list li {{
            padding: 8px 0;
            padding-left: 25px;
            position: relative;
            font-size: 0.9rem;
            color: #495057;
        }}
        
        .action-list li::before {{
            content: '‚Üí';
            position: absolute;
            left: 0;
            color: #667eea;
            font-weight: bold;
        }}
        
        .action-list strong {{
            color: #2c3e50;
        }}
        
        .footer {{
            background: #fafafa;
            padding: 15px 30px;
            text-align: center;
            font-size: 0.85rem;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üìã Code Review Report</h1>
            <div class="meta">{file_path} ‚Ä¢ {current_date}</div>
        </div>
        
        <div class="score-bar">
            <div class="score-item">
                <div class="score-value">{total_issues}</div>
                <div class="score-label">Total Issues</div>
            </div>
            <div class="score-item">
                <div class="score-value" style="color: #c62828;">{severity_counts['Critical']}</div>
                <div class="score-label">Critical</div>
            </div>
            <div class="score-item">
                <div class="score-value" style="color: #e65100;">{severity_counts['High']}</div>
                <div class="score-label">High</div>
            </div>
            <div class="score-item">
                <div class="score-value" style="color: #f57f17;">{severity_counts['Medium']}</div>
                <div class="score-label">Medium</div>
            </div>
            <div class="score-item">
                <div class="score-value" style="color: #2e7d32;">{severity_counts['Low']}</div>
                <div class="score-label">Low</div>
            </div>
        </div>
        
        <div class="content">
            <div class="summary">
                <p><strong>Summary:</strong> {summary_text}</p>
            </div>
            
            <div class="section">
                <h2 class="section-title">üî¥ Critical & High Priority Issues</h2>
                <table class="findings-table">
                    <thead>
                        <tr>
                            <th style="width: 90px;">Severity</th>
                            <th>Issue</th>
                            <th style="width: 100px;">Line</th>
                            <th style="width: 100px;">Fix Time</th>
                        </tr>
                    </thead>
                    <tbody>
                        {critical_high_rows if critical_high_rows else '<tr><td colspan="4" style="text-align: center; color: #6c757d;">No critical or high priority issues found ‚úÖ</td></tr>'}
                    </tbody>
                </table>
            </div>
            
            <div class="section">
                <h2 class="section-title">‚ö° Performance & Code Quality Issues</h2>
                <table class="findings-table">
                    <thead>
                        <tr>
                            <th style="width: 90px;">Severity</th>
                            <th>Issue</th>
                            <th style="width: 100px;">Line</th>
                            <th style="width: 100px;">Impact</th>
                        </tr>
                    </thead>
                    <tbody>
                        {perf_rows if perf_rows else '<tr><td colspan="4" style="text-align: center; color: #6c757d;">No performance issues found ‚úÖ</td></tr>'}
                    </tbody>
                </table>
            </div>
            
            <div class="actions">
                <h2 class="section-title">‚úÖ Required Actions</h2>
                <ul class="action-list">
{action_items}                </ul>
            </div>
        </div>
        
        <div class="footer">
            Generated by Snowflake Cortex AI ({MODEL}) ‚Ä¢ Total estimated fix time: {fix_hours} hours
        </div>
    </div>
</body>
</html>"""
    
    return html_template

# ---------------------
# Main execution
# ---------------------
if __name__ == "__main__":
    try:
        # Read the file
        if not os.path.exists(FILE_TO_REVIEW):
            print(f"‚ùå File {FILE_TO_REVIEW} not found")
            exit(1)
        
        code_text = Path(FILE_TO_REVIEW).read_text()
        print(f"üìÑ Reviewing {FILE_TO_REVIEW} ({len(code_text)} characters)")
        
        # Call Cortex
        print("ü§ñ Getting review from Snowflake Cortex...")
        report = review_with_cortex(MODEL, code_text)
        
        # Print summary of findings
        findings_count = len(report.get("detailed_findings", []))
        print(f"‚úÖ Found {findings_count} issues")
        
        # Generate the HTML report
        print("üìù Generating HTML report...")
        html_report = generate_html_report(report, FILE_TO_REVIEW)
        
        # Save HTML report
        with open("code_review_report.html", "w", encoding='utf-8') as f:
            f.write(html_report)
        print("‚úÖ Generated HTML report: code_review_report.html")
        
        # Save JSON output
        with open("review_output.json", "w") as f:
            json.dump(report, f, indent=2)
        print("‚úÖ Saved JSON to review_output.json")
        
        # Print summary
        print("\n" + "="*50)
        print("üìä REVIEW SUMMARY:")
        print("="*50)
        
        severity_counts = {"Critical": 0, "High": 0, "Medium": 0, "Low": 0}
        for finding in report.get("detailed_findings", []):
            sev = finding.get("severity", "Low")
            if sev in severity_counts:
                severity_counts[sev] += 1
        
        for sev, count in severity_counts.items():
            if count > 0:
                print(f"  {sev}: {count} issues")
        
        print(f"\nüåê Open 'code_review_report.html' in your browser to see the full report!")
        print("="*50)
        
        # Close Snowflake session
        session.close()
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        if 'session' in locals():
            session.close()
        exit(1)
